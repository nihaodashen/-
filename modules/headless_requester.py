"""
æ— å¤´è¯·æ±‚æ¨¡å—
ä½¿ç”¨ cloudscraper ç›´æ¥å‘é€è¯·æ±‚åˆ° LMArena APIï¼Œè‡ªåŠ¨ç»•è¿‡ Cloudflare éªŒè¯
"""

import asyncio
import json
import logging
import uuid
import random
import os
import codecs
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from typing import Optional, AsyncGenerator, Tuple

try:
    import cloudscraper
except ImportError:
    cloudscraper = None
    logging.warning("cloudscraper æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨ Cloudflare ç»•è¿‡åŠŸèƒ½ã€‚è¯·è¿è¡Œ: pip install cloudscraper")

logger = logging.getLogger(__name__)


class ScraperPool:
    """Cloudscraper è¿æ¥æ± ï¼Œæ”¯æŒé«˜å¹¶å‘è¯·æ±‚"""
    
    def __init__(self, pool_size: int, cookies: dict, config: dict):
        """
        åˆå§‹åŒ–è¿æ¥æ± 
        
        Args:
            pool_size: æ± å¤§å°
            cookies: Cookie å­—å…¸
            config: é…ç½®å­—å…¸
        """
        self.pool_size = pool_size
        self.cookies = cookies
        self.config = config
        self._pool = asyncio.Queue(maxsize=pool_size)
        self._lock = asyncio.Lock()
        self._initialized = False
        
    async def initialize(self):
        """åˆå§‹åŒ–è¿æ¥æ± ï¼ˆå¼‚æ­¥ï¼‰"""
        if self._initialized:
            return
        
        async with self._lock:
            if self._initialized:  # åŒé‡æ£€æŸ¥
                return
            
            logger.info(f"æ­£åœ¨åˆå§‹åŒ– Scraper è¿æ¥æ± ï¼ˆå¤§å°: {self.pool_size}ï¼‰...")
            
            for i in range(self.pool_size):
                scraper = self._create_scraper(i)
                await self._pool.put(scraper)
            
            self._initialized = True
            logger.info(f"âœ… Scraper è¿æ¥æ± åˆå§‹åŒ–å®Œæˆï¼ˆ{self.pool_size} ä¸ªå®ä¾‹ï¼‰")
    
    def _create_scraper(self, index: int):
        """
        åˆ›å»ºå•ä¸ª scraper å®ä¾‹
        
        Args:
            index: å®ä¾‹ç´¢å¼•
        """
        if not cloudscraper:
            return None
        
        try:
            # è·å–é…ç½®
            interpreter = self.config.get('cloudscraper_interpreter', 'js2py')
            delay = self.config.get('cloudscraper_delay', 5)
            debug = self.config.get('cloudscraper_debug', False)
            
            # æ¯ä¸ªå®ä¾‹ä½¿ç”¨ä¸åŒçš„æµè§ˆå™¨æŒ‡çº¹
            browser_config = self._get_random_browser_config()
            
            scraper_options = {
                'browser': browser_config,
                'interpreter': interpreter,
                'delay': delay,
                'debug': debug
            }
            
            scraper = cloudscraper.create_scraper(**scraper_options)
            
            # åŠ è½½ cookies
            if self.cookies:
                for name, value in self.cookies.items():
                    scraper.cookies.set(name, value, domain='.lmarena.ai')
            
            # æ ‡è®°å®ä¾‹
            scraper._pool_index = index
            scraper._browser_config = browser_config
            
            logger.debug(f"åˆ›å»º Scraper å®ä¾‹ #{index} (browser: {browser_config['browser']}/{browser_config['platform']})")
            
            return scraper
            
        except Exception as e:
            logger.error(f"åˆ›å»º Scraper å®ä¾‹ #{index} å¤±è´¥: {e}")
            return None
    
    def _get_random_browser_config(self):
        """éšæœºé€‰æ‹©æµè§ˆå™¨é…ç½®"""
        browsers = ['chrome', 'firefox']
        platforms = ['windows', 'linux', 'darwin']
        
        return {
            'browser': random.choice(browsers),
            'platform': random.choice(platforms),
            'mobile': False,
            'desktop': True
        }
    
    async def acquire(self):
        """
        ä»æ± ä¸­è·å–ä¸€ä¸ª scraper
        
        Returns:
            scraper å®ä¾‹
        """
        if not self._initialized:
            await self.initialize()
        
        scraper = await self._pool.get()
        logger.debug(f"è·å– Scraper å®ä¾‹ #{scraper._pool_index if scraper else 'None'} (æ± ä¸­å‰©ä½™: {self._pool.qsize()})")
        return scraper
    
    async def release(self, scraper):
        """
        å½’è¿˜ scraper åˆ°æ± ä¸­
        
        Args:
            scraper: è¦å½’è¿˜çš„ scraper å®ä¾‹
        """
        if scraper:
            await self._pool.put(scraper)
            logger.debug(f"å½’è¿˜ Scraper å®ä¾‹ #{scraper._pool_index} (æ± ä¸­å‰©ä½™: {self._pool.qsize()})")
    
    async def update_cookies(self, cookies: dict):
        """
        æ›´æ–°æ‰€æœ‰ scraper çš„ cookies
        
        Args:
            cookies: æ–°çš„ cookie å­—å…¸
        """
        self.cookies = cookies
        
        # è·å–æ‰€æœ‰ scraperï¼Œæ›´æ–°åå½’è¿˜
        scrapers = []
        for _ in range(self.pool_size):
            scraper = await self.acquire()
            if scraper:
                # æ¸…é™¤æ—§ cookies
                scraper.cookies.clear()
                # è®¾ç½®æ–° cookies
                for name, value in cookies.items():
                    scraper.cookies.set(name, value, domain='.lmarena.ai')
                scrapers.append(scraper)
        
        # å½’è¿˜æ‰€æœ‰ scraper
        for scraper in scrapers:
            await self.release(scraper)
        
        logger.info(f"å·²æ›´æ–°è¿æ¥æ± ä¸­æ‰€æœ‰ {len(scrapers)} ä¸ª Scraper çš„ cookies")


class HeadlessRequester:
    """æ— å¤´è¯·æ±‚å™¨ï¼šä½¿ç”¨ cloudscraper å‘ LMArena API å‘é€è¯·æ±‚ï¼Œè‡ªåŠ¨ç»•è¿‡ Cloudflare"""
    
    def __init__(self, config: dict = None, cookie_getter: callable = None):
        """
        åˆå§‹åŒ–æ— å¤´è¯·æ±‚å™¨
        
        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å« cloudscraper ç›¸å…³é…ç½®
            cookie_getter: cookie è·å–å‡½æ•°ï¼Œè¿”å› dictï¼ˆå¿…éœ€ï¼‰
        """
        self.base_url = "https://lmarena.ai"
        self.config = config or {}
        self.cookies = {}
        self.scraper = None  # ä¿ç•™ç”¨äºå›é€€æ¨¡å¼
        self.request_count = 0
        self._cookies_need_update = False  # Cookie æ›´æ–°æ ‡å¿—
        self.cookie_getter = cookie_getter  # åŠ¨æ€ cookie è·å–å‡½æ•°
        
        # è¿æ¥æ± é…ç½®
        self.use_pool = self.config.get('use_scraper_pool', True)
        pool_size = self.config.get('scraper_pool_size', 10)
        
        # åˆ›å»ºä¸“ç”¨çº¿ç¨‹æ± ç”¨äº cloudscraper åŒæ­¥è°ƒç”¨
        thread_pool_size = self.config.get('thread_pool_size', 50)
        self._executor = ThreadPoolExecutor(
            max_workers=thread_pool_size,
            thread_name_prefix='CloudScraperWorker'
        )
        logger.info(f"âœ… åˆ›å»ºä¸“ç”¨çº¿ç¨‹æ± ï¼ˆå¤§å°: {thread_pool_size}ï¼‰")
        
        # åˆå§‹åŒ– cookies
        self._load_cookies()
        
        if self.use_pool and cloudscraper:
            # ä½¿ç”¨è¿æ¥æ± æ¨¡å¼
            self.scraper_pool = ScraperPool(pool_size, self.cookies, self.config)
            logger.info(f"å¯ç”¨ Scraper è¿æ¥æ± æ¨¡å¼ï¼ˆæ± å¤§å°: {pool_size}ï¼‰")
        else:
            # ä½¿ç”¨å•ä¾‹æ¨¡å¼ï¼ˆå›é€€ï¼‰
            self.scraper_pool = None
            self._init_scraper()
            if not self.use_pool:
                logger.info("å·²ç¦ç”¨è¿æ¥æ± ï¼Œä½¿ç”¨å•ä¾‹æ¨¡å¼")
    
    def _load_cookies(self, force_reload: bool = False):
        """
        ä» .env åŠ è½½ cookiesï¼ˆé€šè¿‡ cookie_getterï¼‰
        
        Args:
            force_reload: å¼ºåˆ¶é‡æ–°åŠ è½½
        """
        if not self.cookie_getter:
            logger.error("âŒ æœªæä¾› cookie_getterï¼Œæ— æ³•åŠ è½½ cookies")
            return False
        
        try:
            dynamic_cookies = self.cookie_getter()
            if dynamic_cookies and isinstance(dynamic_cookies, dict):
                old_count = len(self.cookies)
                self.cookies = dynamic_cookies.copy()
                
                if old_count > 0:
                    logger.debug(f"ğŸ”„ ä» .env é‡æ–°åŠ è½½äº† {len(self.cookies)} ä¸ª cookiesï¼ˆåŸæœ‰ {old_count} ä¸ªï¼‰")
                else:
                    logger.info(f"âœ… ä» .env åŠ è½½äº† {len(self.cookies)} ä¸ª cookies")
                
                self._cookies_need_update = True
                return True
            else:
                logger.warning("âš ï¸ ä» .env è·å–çš„ cookies ä¸ºç©ºæˆ–æ— æ•ˆ")
                return False
        except Exception as e:
            logger.error(f"âŒ ä» .env åŠ è½½ cookies å¤±è´¥: {e}")
            return False
    
    async def _update_scraper_cookies(self):
        """æ›´æ–° scraper çš„ cookiesï¼ˆä»…æ›´æ–°æ± ä¸­çš„ cookies é…ç½®ï¼Œä¸æ›´æ–°æ­£åœ¨ä½¿ç”¨çš„å®ä¾‹ï¼‰"""
        try:
            if self.scraper_pool:
                # åªæ›´æ–°è¿æ¥æ± çš„ cookies é…ç½®ï¼Œä¸è·å–æ‰€æœ‰å®ä¾‹
                # æ–°è·å–çš„ scraper ä¼šè‡ªåŠ¨ä½¿ç”¨æ–° cookies
                self.scraper_pool.cookies = self.cookies.copy()
                logger.debug(f"å·²æ›´æ–°è¿æ¥æ± çš„ cookies é…ç½®ï¼ˆ{len(self.cookies)} ä¸ªï¼‰")
            elif self.scraper:
                # å•ä¾‹æ¨¡å¼ï¼šç›´æ¥æ›´æ–°
                self.scraper.cookies.clear()
                for name, value in self.cookies.items():
                    self.scraper.cookies.set(name, value, domain='.lmarena.ai')
                logger.debug(f"å·²æ›´æ–° scraper çš„ {len(self.cookies)} ä¸ª cookies")
        except Exception as e:
            logger.error(f"æ›´æ–° scraper cookies å¤±è´¥: {e}")
    
    def check_and_reload_cookies(self) -> bool:
        """
        ä» .env é‡æ–°åŠ è½½ cookies
        
        Returns:
            å¦‚æœæˆåŠŸåŠ è½½è¿”å› Trueï¼Œå¦åˆ™è¿”å› False
        """
        return self._load_cookies(force_reload=True)
    
    def force_reload_cookies(self) -> bool:
        """
        å¼ºåˆ¶é‡æ–°åŠ è½½ cookies
        
        Returns:
            å¦‚æœæˆåŠŸé‡æ–°åŠ è½½è¿”å› Trueï¼Œå¦åˆ™è¿”å› False
        """
        logger.info("ğŸ”„ å¼ºåˆ¶é‡æ–°åŠ è½½ cookies...")
        return self._load_cookies(force_reload=True)
    
    def _init_scraper(self):
        """åˆå§‹åŒ– cloudscraper"""
        if not cloudscraper:
            logger.warning("cloudscraper æœªå®‰è£…ï¼Œå°†ä½¿ç”¨åŸºæœ¬çš„è¯·æ±‚åŠŸèƒ½")
            return
        
        try:
            # è·å–é…ç½®
            interpreter = self.config.get('cloudscraper_interpreter', 'js2py')
            delay = self.config.get('cloudscraper_delay', 5)
            debug = self.config.get('cloudscraper_debug', False)
            
            # è·å–æµè§ˆå™¨é…ç½®
            browser_config = self._get_random_browser_config()
            
            # åˆ›å»º scraper - ä½¿ç”¨æ ‡å‡† cloudscraper å‚æ•°
            scraper_options = {
                'browser': browser_config,
                'interpreter': interpreter,
                'delay': delay,
                'debug': debug
            }
            
            self.scraper = cloudscraper.create_scraper(**scraper_options)
            
            # åŠ è½½ cookies åˆ° scraper
            if self.cookies:
                for name, value in self.cookies.items():
                    self.scraper.cookies.set(name, value, domain='.lmarena.ai')
            
            logger.info(f"æˆåŠŸåˆå§‹åŒ– cloudscraper (interpreter: {interpreter}, browser: {browser_config['browser']}/{browser_config['platform']})")
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ– cloudscraper å¤±è´¥: {e}")
            logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            self.scraper = None
    
    def _get_random_browser_config(self):
        """éšæœºé€‰æ‹©æµè§ˆå™¨é…ç½®"""
        browsers = ['chrome', 'firefox']
        platforms = ['windows', 'linux', 'darwin']
        
        return {
            'browser': random.choice(browsers),
            'platform': random.choice(platforms),
            'mobile': False,
            'desktop': True
        }
    
    def _rotate_fingerprint(self):
        """è½®æ¢æµè§ˆå™¨æŒ‡çº¹"""
        if not self.scraper:
            return
        
        try:
            # é‡æ–°åˆ›å»º scraper ä»¥æ›´æ¢æŒ‡çº¹
            logger.info("æ­£åœ¨è½®æ¢æµè§ˆå™¨æŒ‡çº¹...")
            self._init_scraper()
            logger.info("æµè§ˆå™¨æŒ‡çº¹å·²æ›´æ¢")
        except Exception as e:
            logger.error(f"è½®æ¢æŒ‡çº¹å¤±è´¥: {e}")
    
    def _construct_messages(
        self,
        message_templates: list,
        session_id: str,
        is_image_request: bool = False
    ) -> list:
        """
        æ„é€  LMArena API æ‰€éœ€çš„æ¶ˆæ¯æ ¼å¼
        
        Args:
            message_templates: æ¶ˆæ¯æ¨¡æ¿åˆ—è¡¨
            session_id: ä¼šè¯ ID
            is_image_request: æ˜¯å¦ä¸ºå›¾åƒç”Ÿæˆè¯·æ±‚
            
        Returns:
            æ„é€ å¥½çš„æ¶ˆæ¯åˆ—è¡¨
        """
        new_messages = []
        last_msg_id = None
        
        for i, template in enumerate(message_templates):
            current_msg_id = str(uuid.uuid4())
            parent_ids = [last_msg_id] if last_msg_id else []
            
            # å¦‚æœæ˜¯å›¾åƒè¯·æ±‚ï¼Œæ‰€æœ‰æ¶ˆæ¯çŠ¶æ€éƒ½æ˜¯ 'success'
            # å¦åˆ™ï¼Œåªæœ‰æœ€åä¸€æ¡æ¶ˆæ¯æ˜¯ 'pending'
            status = 'success' if is_image_request else ('pending' if i == len(message_templates) - 1 else 'success')
            
            new_messages.append({
                'role': template['role'],
                'content': template.get('content', ''),
                'id': current_msg_id,
                'evaluationId': None,
                'evaluationSessionId': session_id,
                'parentMessageIds': parent_ids,
                'experimental_attachments': template.get('attachments', []),
                'failureReason': None,
                'participantPosition': template.get('participantPosition', 'a'),
                'createdAt': datetime.utcnow().isoformat() + 'Z',
                'updatedAt': datetime.utcnow().isoformat() + 'Z',
                'status': status,
            })
            last_msg_id = current_msg_id
        
        return new_messages
    
    async def send_request(
        self,
        session_id: str,
        message_templates: list,
        target_model_id: Optional[str] = None,
        is_image_request: bool = False
    ) -> AsyncGenerator[Tuple[str, str], None]:
        """
        å‘é€è¯·æ±‚åˆ° LMArena API å¹¶æµå¼è¿”å›ç»“æœ
        
        Args:
            session_id: ä¼šè¯ ID
            message_templates: æ¶ˆæ¯æ¨¡æ¿åˆ—è¡¨
            target_model_id: ç›®æ ‡æ¨¡å‹ IDï¼ˆå¯é€‰ï¼‰
            is_image_request: æ˜¯å¦ä¸ºå›¾åƒç”Ÿæˆè¯·æ±‚
            
        Yields:
            (äº‹ä»¶ç±»å‹, æ•°æ®) å…ƒç»„ï¼Œå…¶ä¸­äº‹ä»¶ç±»å‹å¯ä»¥æ˜¯:
            - 'content': æ–‡æœ¬å†…å®¹
            - 'image': å›¾åƒæ•°æ®
            - 'finish': å®Œæˆä¿¡æ¯
            - 'error': é”™è¯¯ä¿¡æ¯
        """
        # è‡ªåŠ¨ä» .env é‡è½½ cookies
        cookie_reloaded = self.check_and_reload_cookies()
        if cookie_reloaded:
            logger.debug("âœ¨ å·²ä» .env é‡æ–°åŠ è½½ cookies")
            self._cookies_need_update = True
        
        # å¦‚æœ cookies éœ€è¦æ›´æ–°ï¼Œæ›´æ–°æ‰€æœ‰ scraper
        if self._cookies_need_update and (self.scraper_pool or self.scraper):
            await self._update_scraper_cookies()
            self._cookies_need_update = False
        
        # ä»…å¯¹å•ä¾‹æ¨¡å¼è½®æ¢æŒ‡çº¹ï¼ˆè¿æ¥æ± ä¸­æ¯ä¸ªå®ä¾‹éƒ½æœ‰ä¸åŒæŒ‡çº¹ï¼‰
        if not self.scraper_pool:
            fingerprint_rotation_interval = self.config.get('fingerprint_rotation_requests', 10)
            if fingerprint_rotation_interval > 0 and self.request_count > 0 and self.request_count % fingerprint_rotation_interval == 0:
                self._rotate_fingerprint()
        
        self.request_count += 1
        
        # æ„é€  API URL - æ–°æ ¼å¼ï¼š/nextjs-api/stream/post-to-evaluation/{session_id}
        api_url = f"{self.base_url}/nextjs-api/stream/post-to-evaluation/{session_id}"
        
        # ç”Ÿæˆæ‰€éœ€çš„ UUID
        user_message_id = str(uuid.uuid4())
        model_a_message_id = str(uuid.uuid4())
        model_b_message_id = str(uuid.uuid4())
        
        # æ„é€ æ¶ˆæ¯æ•°ç»„
        new_messages = []
        for i, template in enumerate(message_templates):
            message_id = str(uuid.uuid4())
            
            new_messages.append({
                'id': message_id,
                'evaluationSessionId': session_id,
                'role': template['role'],
                'parentMessageIds': [],
                'content': template.get('content', ''),
                'experimental_attachments': template.get('attachments', []),
                'participantPosition': template.get('participantPosition', 'b'),
            })
        
        # æ„é€ æ–°çš„è¯·æ±‚ä½“ç»“æ„
        body = {
            'id': session_id,
            'mode': 'battle',
            'userMessageId': user_message_id,
            'modelAMessageId': model_a_message_id,
            'modelBMessageId': model_b_message_id,
            'messages': new_messages,
            'modality': 'chat'
        }
        
        logger.info(f"[Headless] å‡†å¤‡å‘é€è¯·æ±‚åˆ°: {api_url} (è¯·æ±‚ #{self.request_count})")
        logger.debug(f"[Headless] è¯·æ±‚ä½“: {json.dumps(body, ensure_ascii=False, indent=2)}")
        
        scraper = None
        try:
            # ä½¿ç”¨è¿æ¥æ± æˆ–å•ä¾‹
            if self.scraper_pool and cloudscraper:
                # è¿æ¥æ± æ¨¡å¼
                scraper = await self.scraper_pool.acquire()
                
                if not scraper:
                    logger.error("[Headless] ä»è¿æ¥æ± è·å– scraper å¤±è´¥")
                    yield 'error', "æ— æ³•è·å–å¯ç”¨çš„ scraper å®ä¾‹"
                    return
                
                logger.info(f"[Headless] ä½¿ç”¨è¿æ¥æ±  Scraper #{scraper._pool_index} ({scraper._browser_config['browser']}/{scraper._browser_config['platform']})")
                
                # å‘èµ·è¯·æ±‚ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­ï¼‰
                loop = asyncio.get_event_loop()
                
                # ä»é…ç½®è¯»å–é˜Ÿåˆ—å¤§å°
                queue_size = self.config.get('stream_queue_size', 500)
                chunk_queue = asyncio.Queue(maxsize=queue_size)
                
                def make_request_and_stream():
                    """åœ¨çº¿ç¨‹ä¸­å‘èµ·è¯·æ±‚å¹¶æµå¼è¯»å–"""
                    try:
                        response = scraper.post(
                            api_url,
                            data=json.dumps(body),
                            headers={
                                'Content-Type': 'text/plain;charset=UTF-8',
                                'Accept': '*/*',
                            },
                            stream=True,
                            timeout=360
                        )
                        
                        if response.status_code != 200:
                            error_msg = f"HTTP {response.status_code}: {response.text[:500]}"
                            # ä½¿ç”¨ run_coroutine_threadsafe å®‰å…¨åœ°æ”¾å…¥é˜Ÿåˆ—ï¼ˆä¼šç­‰å¾…ï¼‰
                            future = asyncio.run_coroutine_threadsafe(
                                chunk_queue.put(('error', error_msg)), loop
                            )
                            future.result()  # ç­‰å¾…å®Œæˆ
                            return
                        
                        # æµå¼è¯»å–å¹¶æ”¾å…¥é˜Ÿåˆ—
                        decoder = codecs.getincrementaldecoder('utf-8')(errors='replace')
                        
                        for chunk in response.iter_content(chunk_size=8192, decode_unicode=False):
                            if chunk:
                                text_chunk = decoder.decode(chunk, False)
                                if text_chunk:
                                    # ä½¿ç”¨ run_coroutine_threadsafe å®‰å…¨åœ°æ”¾å…¥é˜Ÿåˆ—ï¼ˆä¼šç­‰å¾…ï¼‰
                                    future = asyncio.run_coroutine_threadsafe(
                                        chunk_queue.put(('chunk', text_chunk)), loop
                                    )
                                    future.result()  # ç­‰å¾…é˜Ÿåˆ—æœ‰ç©ºé—´
                        
                        # è§£ç å‰©ä½™å­—èŠ‚
                        final_chunk = decoder.decode(b'', True)
                        if final_chunk:
                            future = asyncio.run_coroutine_threadsafe(
                                chunk_queue.put(('chunk', final_chunk)), loop
                            )
                            future.result()
                        
                        # æ ‡è®°å®Œæˆ
                        future = asyncio.run_coroutine_threadsafe(
                            chunk_queue.put(('done', None)), loop
                        )
                        future.result()
                        
                    except Exception as e:
                        error_msg = f"è¯·æ±‚å¼‚å¸¸: {str(e)}"
                        try:
                            future = asyncio.run_coroutine_threadsafe(
                                chunk_queue.put(('error', error_msg)), loop
                            )
                            future.result()
                        except:
                            pass  # å¦‚æœè¿é”™è¯¯éƒ½æ”¾ä¸è¿›å»ï¼Œå°±å¿½ç•¥
                
                # åœ¨çº¿ç¨‹æ± ä¸­å¯åŠ¨è¯·æ±‚
                self._executor.submit(make_request_and_stream)
                
                logger.info(f"[Headless] è¯·æ±‚å·²æäº¤ï¼Œç­‰å¾…æ•°æ®...")
                
                # åœ¨ä¸»å¾ªç¯ä¸­ä»é˜Ÿåˆ—è¯»å–æ•°æ®
                while True:
                    msg_type, data = await chunk_queue.get()
                    
                    if msg_type == 'error':
                        logger.error(f"[Headless] è¯·æ±‚å¤±è´¥: {data}")
                        yield 'error', data
                        break
                    elif msg_type == 'chunk':
                        yield 'raw_chunk', data
                    elif msg_type == 'done':
                        logger.info(f"[Headless] æµå¼æ•°æ®æ¥æ”¶å®Œæˆ")
                        break
                
            elif self.scraper and cloudscraper:
                # å•ä¾‹æ¨¡å¼ï¼ˆå›é€€ï¼‰
                logger.info("[Headless] ä½¿ç”¨å•ä¾‹ Scraperï¼ˆè¿æ¥æ± å·²ç¦ç”¨ï¼‰")
                
                loop = asyncio.get_event_loop()
                
                # ä»é…ç½®è¯»å–é˜Ÿåˆ—å¤§å°
                queue_size = self.config.get('stream_queue_size', 500)
                chunk_queue = asyncio.Queue(maxsize=queue_size)
                
                def make_request_and_stream():
                    """åœ¨çº¿ç¨‹ä¸­å‘èµ·è¯·æ±‚å¹¶æµå¼è¯»å–"""
                    try:
                        response = self.scraper.post(
                            api_url,
                            data=json.dumps(body),
                            headers={
                                'Content-Type': 'text/plain;charset=UTF-8',
                                'Accept': '*/*',
                            },
                            stream=True,
                            timeout=360
                        )
                        
                        if response.status_code != 200:
                            error_msg = f"HTTP {response.status_code}: {response.text[:500]}"
                            future = asyncio.run_coroutine_threadsafe(
                                chunk_queue.put(('error', error_msg)), loop
                            )
                            future.result()
                            return
                        
                        decoder = codecs.getincrementaldecoder('utf-8')(errors='replace')
                        
                        for chunk in response.iter_content(chunk_size=8192, decode_unicode=False):
                            if chunk:
                                text_chunk = decoder.decode(chunk, False)
                                if text_chunk:
                                    future = asyncio.run_coroutine_threadsafe(
                                        chunk_queue.put(('chunk', text_chunk)), loop
                                    )
                                    future.result()
                        
                        final_chunk = decoder.decode(b'', True)
                        if final_chunk:
                            future = asyncio.run_coroutine_threadsafe(
                                chunk_queue.put(('chunk', final_chunk)), loop
                            )
                            future.result()
                        
                        future = asyncio.run_coroutine_threadsafe(
                            chunk_queue.put(('done', None)), loop
                        )
                        future.result()
                        
                    except Exception as e:
                        error_msg = f"è¯·æ±‚å¼‚å¸¸: {str(e)}"
                        try:
                            future = asyncio.run_coroutine_threadsafe(
                                chunk_queue.put(('error', error_msg)), loop
                            )
                            future.result()
                        except:
                            pass
                
                # åœ¨çº¿ç¨‹æ± ä¸­å¯åŠ¨è¯·æ±‚
                self._executor.submit(make_request_and_stream)
                
                logger.info(f"[Headless] è¯·æ±‚å·²æäº¤ï¼Œç­‰å¾…æ•°æ®...")
                
                # ä»é˜Ÿåˆ—è¯»å–æ•°æ®
                while True:
                    msg_type, data = await chunk_queue.get()
                    
                    if msg_type == 'error':
                        logger.error(f"[Headless] è¯·æ±‚å¤±è´¥: {data}")
                        yield 'error', data
                        break
                    elif msg_type == 'chunk':
                        yield 'raw_chunk', data
                    elif msg_type == 'done':
                        logger.info(f"[Headless] æµå¼æ•°æ®æ¥æ”¶å®Œæˆ")
                        break
                
            else:
                # å›é€€åˆ°åŸºæœ¬çš„ cookie è¯·æ±‚ï¼ˆä¸ä½¿ç”¨ cloudscraperï¼‰
                logger.warning("[Headless] cloudscraper ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºæœ¬è¯·æ±‚æ¨¡å¼")
                
                if not self.cookies:
                    logger.error("æ²¡æœ‰å¯ç”¨çš„ cookiesï¼Œæ— æ³•å‘é€è¯·æ±‚ã€‚")
                    yield 'error', "æ²¡æœ‰å¯ç”¨çš„ cookies"
                    return
                
                import httpx
                
                headers = {
                    'Content-Type': 'text/plain;charset=UTF-8',
                    'Accept': '*/*',
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Origin': 'https://lmarena.ai',
                    'Referer': 'https://lmarena.ai/',
                }
                
                async with httpx.AsyncClient(cookies=self.cookies, timeout=360.0) as client:
                    async with client.stream(
                        'POST',
                        api_url,
                        headers=headers,
                        content=json.dumps(body),
                    ) as response:
                        
                        if response.status_code != 200:
                            error_text = await response.aread()
                            error_msg = f"HTTP {response.status_code}: {error_text.decode('utf-8', errors='ignore')}"
                            logger.error(f"[Headless] è¯·æ±‚å¤±è´¥: {error_msg}")
                            yield 'error', error_msg
                            return
                        
                        logger.info(f"[Headless] è¯·æ±‚æˆåŠŸï¼Œå¼€å§‹æ¥æ”¶æµå¼æ•°æ®...")
                        
                        # ä½¿ç”¨å¢é‡è§£ç å™¨å¤„ç†æµå¼ UTF-8 æ•°æ®ï¼Œé¿å…å¤šå­—èŠ‚å­—ç¬¦è¢«åˆ‡æ–­
                        decoder = codecs.getincrementaldecoder('utf-8')(errors='replace')
                        
                        async for chunk_bytes in response.aiter_bytes():
                            if chunk_bytes:
                                # ä½¿ç”¨å¢é‡è§£ç å™¨ï¼Œå¯ä»¥æ­£ç¡®å¤„ç†è¢«åˆ‡æ–­çš„å¤šå­—èŠ‚å­—ç¬¦
                                text_chunk = decoder.decode(chunk_bytes, False)
                                if text_chunk:  # åªæœ‰å½“æœ‰å®Œæ•´å­—ç¬¦æ—¶æ‰yield
                                    yield 'raw_chunk', text_chunk
                        
                        # è§£ç å‰©ä½™çš„å­—èŠ‚ï¼ˆå¦‚æœæœ‰ï¼‰
                        final_chunk = decoder.decode(b'', True)
                        if final_chunk:
                            yield 'raw_chunk', final_chunk
                        
                        logger.info(f"[Headless] æµå¼æ•°æ®æ¥æ”¶å®Œæˆã€‚")
                    
        except Exception as e:
            error_msg = f"è¯·æ±‚é”™è¯¯: {str(e)}"
            logger.error(f"[Headless] {error_msg}", exc_info=True)
            yield 'error', error_msg
        finally:
            # å½’è¿˜ scraper åˆ°è¿æ¥æ± 
            if scraper and self.scraper_pool:
                await self.scraper_pool.release(scraper)
                logger.debug(f"å·²å½’è¿˜ Scraper #{scraper._pool_index} åˆ°è¿æ¥æ± ")


# å…¨å±€å®ä¾‹
_headless_requester: Optional[HeadlessRequester] = None


def get_headless_requester(config: dict = None, cookie_getter: callable = None) -> HeadlessRequester:
    """
    è·å–å…¨å±€æ— å¤´è¯·æ±‚å™¨å®ä¾‹
    
    Args:
        config: é…ç½®å­—å…¸
        cookie_getter: cookie è·å–å‡½æ•°ï¼ˆå¿…éœ€ï¼‰
    """
    global _headless_requester
    if _headless_requester is None:
        _headless_requester = HeadlessRequester(config, cookie_getter)
    else:
        # å¦‚æœæä¾›äº†æ–°çš„ cookie_getterï¼Œæ›´æ–°å®ƒ
        if cookie_getter is not None:
            _headless_requester.cookie_getter = cookie_getter
            logger.debug("å·²æ›´æ–° cookie_getter")
    return _headless_requester